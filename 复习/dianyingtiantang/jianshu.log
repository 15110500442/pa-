2018-06-25 17:16:13 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:16:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:16:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:16:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:16:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:16:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:16:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:16:14 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:16:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:16:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:16:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 281,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 16, 14, 974201),
 'log_count/INFO': 7,
 'memusage/max': 773533696,
 'memusage/startup': 773533696,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 16, 14, 241959)}
2018-06-25 17:16:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:16:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:16:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:16:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:16:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:16:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:16:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:16:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:16:39 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:16:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:16:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:16:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 246,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 16, 40, 483118),
 'log_count/INFO': 7,
 'memusage/max': 785936384,
 'memusage/startup': 785936384,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 16, 39, 916646)}
2018-06-25 17:16:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:18:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:18:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:18:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:18:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:18:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:18:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:18:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:18:04 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:18:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:18:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:18:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 281,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 18, 5, 65508),
 'log_count/INFO': 7,
 'memusage/max': 52633600,
 'memusage/startup': 52633600,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 18, 4, 441844)}
2018-06-25 17:18:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:18:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:18:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:18:29 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO'}
2018-06-25 17:18:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:18:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:18:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:18:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:18:29 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:18:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:18:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:18:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 293,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 18, 30, 166189),
 'log_count/INFO': 7,
 'memusage/max': 52527104,
 'memusage/startup': 52527104,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 18, 29, 281815)}
2018-06-25 17:18:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:18:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:18:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:18:50 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:18:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:18:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:18:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:18:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:18:50 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:18:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:18:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:18:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 6669,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 147497,
 'downloader/response_count': 20,
 'downloader/response_status_count/200': 20,
 'dupefilter/filtered': 161,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 18, 53, 148233),
 'log_count/INFO': 7,
 'memusage/max': 52776960,
 'memusage/startup': 52776960,
 'offsite/domains': 1,
 'offsite/filtered': 64,
 'request_depth_max': 4,
 'response_received_count': 19,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'start_time': datetime.datetime(2018, 6, 25, 9, 18, 50, 586071)}
2018-06-25 17:18:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:21:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:21:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:21:52 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:21:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:21:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:21:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:21:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:21:52 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:21:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:21:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:21:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 276,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 21, 53, 167660),
 'log_count/INFO': 7,
 'memusage/max': 52563968,
 'memusage/startup': 52563968,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 21, 52, 608339)}
2018-06-25 17:21:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:22:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:22:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:22:22 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:22:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:22:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:22:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:22:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:22:22 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:22:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:22:23 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:22:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 274,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 22, 23, 692099),
 'log_count/INFO': 7,
 'memusage/max': 52690944,
 'memusage/startup': 52690944,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 22, 22, 802717)}
2018-06-25 17:22:23 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:23:01 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:23:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:23:01 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:23:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:23:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:23:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:23:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:23:01 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:23:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:23:02 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:23:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 287,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 23, 2, 755951),
 'log_count/INFO': 7,
 'memusage/max': 52625408,
 'memusage/startup': 52625408,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 23, 1, 798811)}
2018-06-25 17:23:02 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:23:32 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:23:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:23:32 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 17:23:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:23:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:23:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:23:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:23:32 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:23:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:23:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:23:33 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:23:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 23, 33, 764472),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52576256,
 'memusage/startup': 52576256,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 23, 32, 211983)}
2018-06-25 17:23:33 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:24:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:24:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:24:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO'}
2018-06-25 17:24:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:24:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:24:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:24:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:24:24 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:24:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:24:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:24:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:24:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 282,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 24, 25, 80512),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52518912,
 'memusage/startup': 52518912,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 24, 24, 406678)}
2018-06-25 17:24:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:24:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:24:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:24:58 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:24:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:24:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:24:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:24:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:24:58 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:24:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:24:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:24:59 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:24:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 281,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 24, 59, 344581),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52686848,
 'memusage/startup': 52686848,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 24, 58, 776355)}
2018-06-25 17:24:59 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:25:46 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:25:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:25:46 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:25:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:25:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:25:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:25:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:25:46 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:25:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:25:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div[@class="contain"]/ul/
2018-06-25 17:25:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:25:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 282,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 25, 47, 865706),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52596736,
 'memusage/startup': 52596736,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 25, 46, 788757)}
2018-06-25 17:25:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:26:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:26:02 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:26:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:26:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:26:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:26:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:26:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:26:02 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:26:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:26:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div[@class="contain"]/ul/
2018-06-25 17:26:02 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:26:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 241,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 26, 2, 742163),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52670464,
 'memusage/startup': 52670464,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 26, 2, 186765)}
2018-06-25 17:26:02 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:26:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:26:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:26:18 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO'}
2018-06-25 17:26:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:26:18 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:26:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:26:20 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:26:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 828,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 26, 20, 705249),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52727808,
 'memusage/startup': 52727808,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 9, 26, 18, 330572)}
2018-06-25 17:26:20 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:26:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:26:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:26:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO'}
2018-06-25 17:26:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:26:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:26:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:26:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:26:33 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:26:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:26:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:26:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:26:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 876,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 26, 35, 893864),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52699136,
 'memusage/startup': 52699136,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 9, 26, 33, 466988)}
2018-06-25 17:26:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:26:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:26:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:26:43 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:26:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:26:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:26:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:26:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:26:43 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:26:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:26:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:26:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:26:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 822,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 26, 46, 516444),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52490240,
 'memusage/startup': 52490240,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 9, 26, 43, 929378)}
2018-06-25 17:26:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:27:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:27:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:27:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:27:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:27:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:27:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:27:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:27:08 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:27:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:27:11 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:27:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:27:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 876,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 27, 11, 272670),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52563968,
 'memusage/startup': 52563968,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 9, 27, 8, 380220)}
2018-06-25 17:27:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:27:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:27:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:27:40 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO'}
2018-06-25 17:27:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:27:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:27:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:27:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:27:40 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:27:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:27:41 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:27:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:27:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 333,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 27, 41, 572974),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52187136,
 'memusage/startup': 52187136,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 27, 40, 958668)}
2018-06-25 17:27:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:28:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:28:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:28:31 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO'}
2018-06-25 17:28:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:28:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:28:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:28:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:28:31 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:28:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:28:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:28:32 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:28:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 281,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 28, 32, 695268),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52633600,
 'memusage/startup': 52633600,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 28, 31, 473231)}
2018-06-25 17:28:32 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:28:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:28:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:28:52 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:28:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:28:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:28:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:28:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:28:52 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:28:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //*[@id="menu"]/div/ul/
2018-06-25 17:28:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:28:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 292,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 28, 53, 589951),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52654080,
 'memusage/startup': 52654080,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 28, 52, 427618)}
2018-06-25 17:28:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:29:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:29:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:29:28 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:29:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:29:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:29:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:29:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:29:28 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:29:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:29:29 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //div[@id="menu"]/div/ul/
2018-06-25 17:29:29 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:29:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 282,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 29, 29, 427194),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52600832,
 'memusage/startup': 52600832,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 29, 28, 664125)}
2018-06-25 17:29:29 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:32:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:32:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:32:00 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:32:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:32:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:32:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:32:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:32:01 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:32:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:32:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:32:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:32:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:32:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 17:32:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:32:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:32:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:32:17 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:32:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:32:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:32:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:32:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:32:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:32:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:32:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:32:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:32:37 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:32:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:32:56 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:32:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 117374,
 'downloader/request_count': 344,
 'downloader/request_method_count/GET': 344,
 'downloader/response_bytes': 1999747,
 'downloader/response_count': 344,
 'downloader/response_status_count/200': 344,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 32, 56, 337342),
 'log_count/INFO': 7,
 'memusage/max': 52641792,
 'memusage/startup': 52641792,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 343,
 'scheduler/dequeued': 344,
 'scheduler/dequeued/memory': 344,
 'scheduler/enqueued': 344,
 'scheduler/enqueued/memory': 344,
 'start_time': datetime.datetime(2018, 6, 25, 9, 32, 37, 547481)}
2018-06-25 17:32:56 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:35:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:35:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:35:25 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:35:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:35:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:35:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:35:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:35:25 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:35:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:35:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/> (referer: None)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 82, in _parse_response
    for request_or_item in self._requests_to_follow(response):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 61, in _requests_to_follow
    links = [lnk for lnk in rule.link_extractor.extract_links(response)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 122, in extract_links
    for x in self.restrict_xpaths
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/linkextractors/lxmlhtml.py", line 123, in <listcomp>
    for subdoc in response.xpath(x)]
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/http/response/text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/parsel/selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in //div[@id="header"]/
2018-06-25 17:35:26 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:35:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 281,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 35, 26, 604924),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52289536,
 'memusage/startup': 52289536,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 35, 25, 608854)}
2018-06-25 17:35:26 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:35:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:35:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:35:43 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:35:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:35:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:35:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:35:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:35:43 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:35:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:36:32 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:36:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:36:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO'}
2018-06-25 17:36:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 17:36:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:36:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:36:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:36:33 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:36:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:36:33 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:36:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 275,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11476,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 36, 33, 988673),
 'log_count/INFO': 7,
 'memusage/max': 52637696,
 'memusage/startup': 52637696,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 25, 9, 36, 33, 58718)}
2018-06-25 17:36:33 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:38:32 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:38:32 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:38:32 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:38:37 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:38:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7057,
 'downloader/request_count': 20,
 'downloader/request_method_count/GET': 20,
 'downloader/response_bytes': 147713,
 'downloader/response_count': 20,
 'downloader/response_status_count/200': 20,
 'dupefilter/filtered': 161,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 38, 37, 59192),
 'log_count/INFO': 7,
 'memusage/max': 52371456,
 'memusage/startup': 52371456,
 'offsite/domains': 1,
 'offsite/filtered': 64,
 'request_depth_max': 4,
 'response_received_count': 19,
 'scheduler/dequeued': 20,
 'scheduler/dequeued/memory': 20,
 'scheduler/enqueued': 20,
 'scheduler/enqueued/memory': 20,
 'start_time': datetime.datetime(2018, 6, 25, 9, 38, 32, 341076)}
2018-06-25 17:38:37 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:41:07 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:41:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:41:07 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:41:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:41:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:41:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:41:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:41:07 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:41:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:41:10 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/index.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/dyzz/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:41:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:41:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 6,
 'downloader/request_bytes': 8111,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 128761,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'dupefilter/filtered': 141,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 41, 11, 406682),
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 52654080,
 'memusage/startup': 52654080,
 'offsite/domains': 1,
 'offsite/filtered': 56,
 'request_depth_max': 4,
 'response_received_count': 17,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2018, 6, 25, 9, 41, 7, 235851)}
2018-06-25 17:41:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:46:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:46:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:46:43 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 17:46:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:46:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:46:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:46:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:46:43 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:46:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:46:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/oumeitv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/game/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/2009zongyi/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/jddy/20160320/50523.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/zongyi2013/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/dongman/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/newgame/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/hytv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/rihan/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:46:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 30,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 30,
 'downloader/request_bytes': 9456,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 11692,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 46, 47, 922578),
 'log_count/ERROR': 10,
 'log_count/INFO': 7,
 'memusage/max': 824147968,
 'memusage/startup': 824147968,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 20,
 'retry/max_reached': 10,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 20,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2018, 6, 25, 9, 46, 43, 234912)}
2018-06-25 17:46:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:46:54 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:46:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:46:54 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 17:46:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:46:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:46:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:46:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:46:54 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:46:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:46:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/rihan/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/dongman/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/oumeitv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:46:59 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/newgame/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/hytv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/game/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/zongyi2013/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/jddy/20160320/50523.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/2009zongyi/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:00 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:47:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 30,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 30,
 'downloader/request_bytes': 8960,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 11692,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 47, 0, 676654),
 'log_count/ERROR': 10,
 'log_count/INFO': 7,
 'memusage/max': 52768768,
 'memusage/startup': 52768768,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 20,
 'retry/max_reached': 10,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 20,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2018, 6, 25, 9, 46, 54, 888927)}
2018-06-25 17:47:00 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:47:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:47:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:47:23 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 17:47:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:47:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:47:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:47:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:47:23 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:47:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:47:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/2009zongyi/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/jddy/20160320/50523.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/dongman/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:26 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/newgame/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/game/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/zongyi2013/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/hytv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/gndy/rihan/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/html/tv/oumeitv/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:47:27 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:47:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 30,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 30,
 'downloader/request_bytes': 11533,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 11692,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 47, 27, 436555),
 'log_count/ERROR': 10,
 'log_count/INFO': 7,
 'memusage/max': 52772864,
 'memusage/startup': 52772864,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 20,
 'retry/max_reached': 10,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 20,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2018, 6, 25, 9, 47, 23, 585918)}
2018-06-25 17:47:27 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:48:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:48:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:48:58 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 17:48:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:48:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:48:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:48:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:48:58 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:48:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:56:01 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:56:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:56:01 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 17:56:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 17:56:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:56:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:56:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:56:01 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:56:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:56:08 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/index.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:58:07 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:58:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:58:07 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 17:58:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:58:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:58:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:58:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:58:07 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:58:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 17:58:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 17:58:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 17:58:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 819,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 9, 58, 9, 875270),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52637696,
 'memusage/startup': 52637696,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 9, 58, 7, 541367)}
2018-06-25 17:58:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 17:59:46 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 17:59:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 17:59:46 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 17:59:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 17:59:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 17:59:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 17:59:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 17:59:46 [scrapy.core.engine] INFO: Spider opened
2018-06-25 17:59:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:11:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:11:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:11:59 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 18:11:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 18:11:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:11:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:11:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:11:59 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:11:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:12:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://dytt8.net/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-06-25 18:12:01 [scrapy.core.engine] INFO: Closing spider (finished)
2018-06-25 18:12:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 846,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 25, 10, 12, 1, 858924),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52584448,
 'memusage/startup': 52584448,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 25, 10, 11, 59, 170075)}
2018-06-25 18:12:01 [scrapy.core.engine] INFO: Spider closed (finished)
2018-06-25 18:14:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:14:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:14:55 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO'}
2018-06-25 18:14:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 18:14:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:14:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:14:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:14:55 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:14:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:17:51 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:17:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:17:51 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 18:17:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 18:17:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:17:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:17:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:17:51 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:17:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:29:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:29:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:29:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log'}
2018-06-25 18:29:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 18:29:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:29:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:29:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:29:05 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:29:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:35:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:35:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:35:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 18:35:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 18:35:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:35:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:35:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:35:21 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:35:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:38:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:38:02 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:38:02 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO'}
2018-06-25 18:38:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 18:38:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:38:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:38:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:38:02 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:38:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:43:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:43:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:43:15 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 18:43:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 18:43:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:43:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:43:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:43:15 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:43:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:44:42 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:44:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:44:42 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 18:44:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 18:44:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:44:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:44:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:44:42 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:44:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:45:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:45:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:45:19 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 18:45:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 18:45:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:45:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:45:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:45:19 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:45:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:45:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:45:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:45:35 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang'}
2018-06-25 18:45:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 18:45:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:45:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:45:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:45:35 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:45:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:46:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:46:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:46:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 18:46:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-06-25 18:46:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:46:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:46:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:46:22 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:46:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:47:13 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:47:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:47:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 18:47:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 18:47:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:47:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:47:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:47:13 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:47:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:47:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:47:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:47:36 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log'}
2018-06-25 18:47:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 18:47:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:47:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:47:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:47:36 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:47:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:51:57 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:51:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:51:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log', 'LOG_LEVEL': 'INFO'}
2018-06-25 18:51:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 18:51:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:51:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:51:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:51:58 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:51:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:52:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:52:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:52:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 18:52:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 18:52:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:52:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:52:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:52:11 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:52:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 18:57:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 18:57:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 18:57:59 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log'}
2018-06-25 18:57:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 18:57:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 18:57:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 18:57:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 18:57:59 [scrapy.core.engine] INFO: Spider opened
2018-06-25 18:57:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:01:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:01:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:01:23 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log'}
2018-06-25 19:01:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 19:01:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:01:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:01:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:01:23 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:01:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:01:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:01:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:01:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO'}
2018-06-25 19:01:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 19:01:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:01:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:01:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:01:41 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:01:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:01:56 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:01:56 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:01:56 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_FILE': 'jianshu.log'}
2018-06-25 19:01:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 19:01:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:01:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:01:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:01:56 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:01:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/dyzz/index.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180126/56195.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180127/56203.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180127/56201.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180127/56204.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180127/56202.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180124/56181.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180124/56182.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180125/56184.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180125/56187.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180125/56186.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180125/56185.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180126/56192.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20170922/55087.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20170926/55118.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20170926/55119.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20171013/55288.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20180330/56596.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/tv/oumeitv/20180531/56917.html> (referer: http://dytt8.net/html/tv/oumeitv/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180411/56692.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180414/56702.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180413/56698.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180606/56973.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 45, in parse_dytt_detail
    title = response.xpath('//div[@class="co_content8"]/div[@id="Zoom"]/span/p[1]/text()').extract()[0]
IndexError: list index out of range
2018-06-25 19:02:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:02:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:02:17 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_FILE': 'jianshu.log', 'BOT_NAME': 'dianyingtiantang', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO'}
2018-06-25 19:02:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-06-25 19:02:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:02:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:02:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:02:17 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:02:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:02:54 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:02:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:02:54 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 19:02:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 19:02:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:02:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:02:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:02:54 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:02:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:06:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:06:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:06:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders', 'LOG_LEVEL': 'INFO', 'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders']}
2018-06-25 19:06:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2018-06-25 19:06:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:06:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:06:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:06:28 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:06:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-06-25 19:06:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180126/56195.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180127/56202.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/newgame/20180124/56181.html> (referer: http://dytt8.net/)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:41 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180411/56692.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:41 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180413/56698.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:42 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180414/56702.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:06:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dytt8.net/html/gndy/jddy/20180606/56973.html> (referer: http://dytt8.net/html/gndy/rihan/index.html)
Traceback (most recent call last):
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/bc/.virtualenvs/scrapylenv/lib/python3.5/site-packages/scrapy/spiders/crawl.py", line 76, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
  File "/home/bc//pachong/dianyingtiantang/dianyingtiantang/spiders/dytt.py", line 47, in parse_dytt_detail
    f.write(response)
TypeError: write() argument must be str, not HtmlResponse
2018-06-25 19:07:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: dianyingtiantang)
2018-06-25 19:07:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
2018-06-25 19:07:31 [scrapy.crawler] INFO: Overridden settings: {'LOG_FILE': 'jianshu.log', 'SPIDER_MODULES': ['dianyingtiantang.spiders'], 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'dianyingtiantang', 'NEWSPIDER_MODULE': 'dianyingtiantang.spiders'}
2018-06-25 19:07:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2018-06-25 19:07:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-06-25 19:07:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-06-25 19:07:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-06-25 19:07:31 [scrapy.core.engine] INFO: Spider opened
2018-06-25 19:07:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
